{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30de345",
   "metadata": {},
   "source": [
    "# Neural Network Predictive Model\n",
    "* In this notebook we will construct neural networks in order to predict the valence of songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b666e3",
   "metadata": {
    "id": "Tj4ZML0N1FWh"
   },
   "source": [
    "## Import packages\n",
    "* To begin with, we will import the packages, that we will use in the following segments of the project:\n",
    "    * [pandas](https://pandas.pydata.org/)\n",
    "    * [NumPy](https://www.numpy.org/)\n",
    "    * [TensorFlow](https://www.tensorflow.org/)\n",
    "    * [Keras](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
    "* Note that the prementioned packages **must be locally installed too** in order to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a45864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential, Input, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9209f49",
   "metadata": {},
   "source": [
    "## Datasets Loading\n",
    "* We will also load the datasets that we created in the <code>non_nn_predictive.ipynb</code> notebook, as we will use them to train and evaluate our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae19a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_train = pd.read_csv('data/datasets/all/all_X_train.csv', index_col=0)\n",
    "all_X_test = pd.read_csv('data/datasets/all/all_X_test.csv', index_col=0)\n",
    "imp_X_train = pd.read_csv('data/datasets/important/imp_X_train.csv', index_col=0)\n",
    "imp_X_test = pd.read_csv('data/datasets/important/imp_X_test.csv', index_col=0)\n",
    "y_train = pd.read_csv('data/datasets/target/y_train.csv', index_col=0)\n",
    "y_test = pd.read_csv('data/datasets/target/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de8b5f",
   "metadata": {
    "id": "1kc7MQWYn7Z2"
   },
   "source": [
    "## Important-Columns Network\n",
    "* In our first Network, we will use the important-dataset.\n",
    "* It will be a simple one, and it will be constructed using only three **Dense Layers** and one for **Batch Normalization**.\n",
    "* So let's create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a64754d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2bka-WIoaap",
    "outputId": "7684a91b-1224-4c9d-fb25-308112f7794b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 36)               144       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1184      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,873\n",
      "Trainable params: 1,801\n",
      "Non-trainable params: 72\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imp_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32, 'sigmoid'),\n",
    "    tf.keras.layers.Dense(16, activation='softmax'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "imp_model.build((None, 36))\n",
    "imp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa0baa",
   "metadata": {
    "id": "IykvwJnaozY6"
   },
   "source": [
    "* Next, we will compile it using the **Adam** Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b809f2",
   "metadata": {
    "id": "an16w6nno477"
   },
   "outputs": [],
   "source": [
    "imp_model.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd898b",
   "metadata": {
    "id": "A5QkO5OZpAvv"
   },
   "source": [
    "* For our train we will use the [ReduceLROnPlateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau) callback, that reduces the learning rate of the model during the train when it is stack.\n",
    "* We will also use the [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) callback that stops the train of the model when no improvement is noticed for many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34c10b8",
   "metadata": {
    "id": "IwCebtWlpUF6"
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, mode='min', patience=15, min_lr=0.00005)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c856ce0",
   "metadata": {
    "id": "mDm5X2f1p-DZ"
   },
   "source": [
    "* Now we are ready to train the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e254ef3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PE_m8UtqBbw",
    "outputId": "322c9d07-df85-4637-adc5-9a18e0ffe273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "619/619 [==============================] - 2s 2ms/step - loss: 0.1583 - val_loss: 0.1337 - lr: 0.0010\n",
      "Epoch 2/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1298 - val_loss: 0.1248 - lr: 0.0010\n",
      "Epoch 3/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1228 - val_loss: 0.1196 - lr: 0.0010\n",
      "Epoch 4/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1185 - val_loss: 0.1153 - lr: 0.0010\n",
      "Epoch 5/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1148 - val_loss: 0.1120 - lr: 0.0010\n",
      "Epoch 6/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1121 - val_loss: 0.1089 - lr: 0.0010\n",
      "Epoch 7/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1101 - val_loss: 0.1064 - lr: 0.0010\n",
      "Epoch 8/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1085 - val_loss: 0.1055 - lr: 0.0010\n",
      "Epoch 9/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1072 - val_loss: 0.1043 - lr: 0.0010\n",
      "Epoch 10/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1056 - val_loss: 0.1026 - lr: 0.0010\n",
      "Epoch 11/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1050 - val_loss: 0.1009 - lr: 0.0010\n",
      "Epoch 12/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1043 - val_loss: 0.1003 - lr: 0.0010\n",
      "Epoch 13/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1039 - val_loss: 0.0998 - lr: 0.0010\n",
      "Epoch 14/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1031 - val_loss: 0.0995 - lr: 0.0010\n",
      "Epoch 15/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1026 - val_loss: 0.0990 - lr: 0.0010\n",
      "Epoch 16/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1021 - val_loss: 0.0986 - lr: 0.0010\n",
      "Epoch 17/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1015 - val_loss: 0.0977 - lr: 0.0010\n",
      "Epoch 18/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1011 - val_loss: 0.0978 - lr: 0.0010\n",
      "Epoch 19/400\n",
      "619/619 [==============================] - 1s 998us/step - loss: 0.1010 - val_loss: 0.0977 - lr: 0.0010\n",
      "Epoch 20/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1009 - val_loss: 0.0977 - lr: 0.0010\n",
      "Epoch 21/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1007 - val_loss: 0.0969 - lr: 0.0010\n",
      "Epoch 22/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1003 - val_loss: 0.0967 - lr: 0.0010\n",
      "Epoch 23/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0996 - val_loss: 0.0969 - lr: 0.0010\n",
      "Epoch 24/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0996 - val_loss: 0.0973 - lr: 0.0010\n",
      "Epoch 25/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.1003 - val_loss: 0.0965 - lr: 0.0010\n",
      "Epoch 26/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0990 - val_loss: 0.0977 - lr: 0.0010\n",
      "Epoch 27/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0995 - val_loss: 0.0963 - lr: 0.0010\n",
      "Epoch 28/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0988 - val_loss: 0.0977 - lr: 0.0010\n",
      "Epoch 29/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0987 - val_loss: 0.0964 - lr: 0.0010\n",
      "Epoch 30/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0989 - val_loss: 0.0959 - lr: 0.0010\n",
      "Epoch 31/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0983 - val_loss: 0.0963 - lr: 0.0010\n",
      "Epoch 32/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0986 - val_loss: 0.0970 - lr: 0.0010\n",
      "Epoch 33/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0984 - val_loss: 0.0955 - lr: 0.0010\n",
      "Epoch 34/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0982 - val_loss: 0.0960 - lr: 0.0010\n",
      "Epoch 35/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0981 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 36/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0977 - val_loss: 0.0959 - lr: 0.0010\n",
      "Epoch 37/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0974 - val_loss: 0.0955 - lr: 0.0010\n",
      "Epoch 38/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0970 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 39/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0970 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 40/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0974 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 41/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0973 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 42/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0972 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 43/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0970 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 44/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0972 - val_loss: 0.0957 - lr: 0.0010\n",
      "Epoch 45/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0969 - val_loss: 0.0950 - lr: 0.0010\n",
      "Epoch 46/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0967 - val_loss: 0.0954 - lr: 0.0010\n",
      "Epoch 47/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0971 - val_loss: 0.0965 - lr: 0.0010\n",
      "Epoch 48/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0965 - val_loss: 0.0947 - lr: 0.0010\n",
      "Epoch 49/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0973 - val_loss: 0.0945 - lr: 0.0010\n",
      "Epoch 50/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0962 - val_loss: 0.0947 - lr: 0.0010\n",
      "Epoch 51/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0965 - val_loss: 0.0950 - lr: 0.0010\n",
      "Epoch 52/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0961 - val_loss: 0.0950 - lr: 0.0010\n",
      "Epoch 53/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0963 - val_loss: 0.0946 - lr: 0.0010\n",
      "Epoch 54/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0960 - val_loss: 0.0949 - lr: 0.0010\n",
      "Epoch 55/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0960 - val_loss: 0.0951 - lr: 0.0010\n",
      "Epoch 56/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0963 - val_loss: 0.0948 - lr: 0.0010\n",
      "Epoch 57/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0958 - val_loss: 0.0947 - lr: 0.0010\n",
      "Epoch 58/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0956 - val_loss: 0.0946 - lr: 0.0010\n",
      "Epoch 59/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0962 - val_loss: 0.0946 - lr: 0.0010\n",
      "Epoch 60/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0954 - val_loss: 0.0945 - lr: 0.0010\n",
      "Epoch 61/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0956 - val_loss: 0.0945 - lr: 0.0010\n",
      "Epoch 62/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0958 - val_loss: 0.0942 - lr: 0.0010\n",
      "Epoch 63/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0962 - val_loss: 0.0961 - lr: 0.0010\n",
      "Epoch 64/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0956 - val_loss: 0.0944 - lr: 0.0010\n",
      "Epoch 65/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0959 - val_loss: 0.0964 - lr: 0.0010\n",
      "Epoch 66/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0956 - val_loss: 0.0946 - lr: 0.0010\n",
      "Epoch 67/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0953 - val_loss: 0.0941 - lr: 0.0010\n",
      "Epoch 68/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0940 - lr: 0.0010\n",
      "Epoch 69/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0952 - val_loss: 0.0943 - lr: 0.0010\n",
      "Epoch 70/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0947 - lr: 0.0010\n",
      "Epoch 71/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0948 - val_loss: 0.0941 - lr: 0.0010\n",
      "Epoch 72/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0959 - val_loss: 0.0944 - lr: 0.0010\n",
      "Epoch 73/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0953 - val_loss: 0.0949 - lr: 0.0010\n",
      "Epoch 74/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0950 - val_loss: 0.0941 - lr: 0.0010\n",
      "Epoch 75/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0948 - lr: 0.0010\n",
      "Epoch 76/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0943 - lr: 0.0010\n",
      "Epoch 77/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0950 - val_loss: 0.0937 - lr: 0.0010\n",
      "Epoch 78/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0953 - val_loss: 0.0943 - lr: 0.0010\n",
      "Epoch 79/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0953 - val_loss: 0.0942 - lr: 0.0010\n",
      "Epoch 80/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0938 - lr: 0.0010\n",
      "Epoch 81/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0952 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 82/400\n",
      "619/619 [==============================] - 1s 997us/step - loss: 0.0948 - val_loss: 0.0937 - lr: 0.0010\n",
      "Epoch 83/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0945 - val_loss: 0.0933 - lr: 0.0010\n",
      "Epoch 84/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0941 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 85/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0948 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 86/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0956 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 87/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0953 - val_loss: 0.0937 - lr: 0.0010\n",
      "Epoch 88/400\n",
      "619/619 [==============================] - 1s 985us/step - loss: 0.0953 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 89/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0951 - val_loss: 0.0929 - lr: 0.0010\n",
      "Epoch 90/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0944 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 91/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0948 - val_loss: 0.0937 - lr: 0.0010\n",
      "Epoch 92/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0945 - val_loss: 0.0933 - lr: 0.0010\n",
      "Epoch 93/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0948 - val_loss: 0.0943 - lr: 0.0010\n",
      "Epoch 94/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0950 - val_loss: 0.0940 - lr: 0.0010\n",
      "Epoch 95/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0946 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 96/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0942 - val_loss: 0.0933 - lr: 0.0010\n",
      "Epoch 97/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0946 - val_loss: 0.0938 - lr: 0.0010\n",
      "Epoch 98/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0946 - val_loss: 0.0931 - lr: 0.0010\n",
      "Epoch 99/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0947 - val_loss: 0.0930 - lr: 0.0010\n",
      "Epoch 100/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0944 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 101/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0944 - val_loss: 0.0924 - lr: 0.0010\n",
      "Epoch 102/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0942 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 103/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0946 - val_loss: 0.0955 - lr: 0.0010\n",
      "Epoch 104/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0943 - val_loss: 0.0928 - lr: 0.0010\n",
      "Epoch 105/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0944 - val_loss: 0.0930 - lr: 0.0010\n",
      "Epoch 106/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0942 - val_loss: 0.0929 - lr: 0.0010\n",
      "Epoch 107/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0941 - val_loss: 0.0925 - lr: 0.0010\n",
      "Epoch 108/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0939 - val_loss: 0.0926 - lr: 0.0010\n",
      "Epoch 109/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0939 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 110/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0939 - val_loss: 0.0931 - lr: 0.0010\n",
      "Epoch 111/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0941 - val_loss: 0.0930 - lr: 0.0010\n",
      "Epoch 112/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0942 - val_loss: 0.0936 - lr: 0.0010\n",
      "Epoch 113/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0943 - val_loss: 0.0930 - lr: 0.0010\n",
      "Epoch 114/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0941 - val_loss: 0.0932 - lr: 0.0010\n",
      "Epoch 115/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0944 - val_loss: 0.0930 - lr: 0.0010\n",
      "Epoch 116/400\n",
      "619/619 [==============================] - 1s 990us/step - loss: 0.0946 - val_loss: 0.0928 - lr: 0.0010\n",
      "Epoch 117/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0939 - val_loss: 0.0927 - lr: 7.0000e-04\n",
      "Epoch 118/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0932 - lr: 7.0000e-04\n",
      "Epoch 119/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0936 - val_loss: 0.0925 - lr: 7.0000e-04\n",
      "Epoch 120/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 121/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0923 - lr: 7.0000e-04\n",
      "Epoch 122/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0924 - lr: 7.0000e-04\n",
      "Epoch 123/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0933 - lr: 7.0000e-04\n",
      "Epoch 124/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0928 - lr: 7.0000e-04\n",
      "Epoch 125/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0931 - val_loss: 0.0923 - lr: 7.0000e-04\n",
      "Epoch 126/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 127/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0941 - val_loss: 0.0923 - lr: 7.0000e-04\n",
      "Epoch 128/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0921 - lr: 7.0000e-04\n",
      "Epoch 129/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0942 - lr: 7.0000e-04\n",
      "Epoch 130/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0938 - val_loss: 0.0926 - lr: 7.0000e-04\n",
      "Epoch 131/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0921 - lr: 7.0000e-04\n",
      "Epoch 132/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0935 - lr: 7.0000e-04\n",
      "Epoch 133/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0931 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 134/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 135/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 136/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0923 - lr: 7.0000e-04\n",
      "Epoch 137/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0929 - lr: 7.0000e-04\n",
      "Epoch 138/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0931 - val_loss: 0.0918 - lr: 7.0000e-04\n",
      "Epoch 139/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0925 - lr: 7.0000e-04\n",
      "Epoch 140/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0930 - lr: 7.0000e-04\n",
      "Epoch 141/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0930 - lr: 7.0000e-04\n",
      "Epoch 142/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0925 - lr: 7.0000e-04\n",
      "Epoch 143/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0921 - lr: 7.0000e-04\n",
      "Epoch 144/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0933 - lr: 7.0000e-04\n",
      "Epoch 145/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 146/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0933 - val_loss: 0.0920 - lr: 7.0000e-04\n",
      "Epoch 147/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0921 - lr: 7.0000e-04\n",
      "Epoch 148/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0934 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 149/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0927 - lr: 7.0000e-04\n",
      "Epoch 150/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0924 - lr: 7.0000e-04\n",
      "Epoch 151/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0937 - val_loss: 0.0930 - lr: 7.0000e-04\n",
      "Epoch 152/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0923 - lr: 7.0000e-04\n",
      "Epoch 153/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0922 - lr: 7.0000e-04\n",
      "Epoch 154/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0921 - lr: 4.9000e-04\n",
      "Epoch 155/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0925 - val_loss: 0.0923 - lr: 4.9000e-04\n",
      "Epoch 156/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0923 - lr: 4.9000e-04\n",
      "Epoch 157/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0922 - lr: 4.9000e-04\n",
      "Epoch 158/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0919 - lr: 4.9000e-04\n",
      "Epoch 159/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0925 - val_loss: 0.0921 - lr: 4.9000e-04\n",
      "Epoch 160/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0917 - lr: 4.9000e-04\n",
      "Epoch 161/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0919 - lr: 4.9000e-04\n",
      "Epoch 162/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0928 - val_loss: 0.0928 - lr: 4.9000e-04\n",
      "Epoch 163/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0935 - val_loss: 0.0919 - lr: 4.9000e-04\n",
      "Epoch 164/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0922 - lr: 4.9000e-04\n",
      "Epoch 165/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0930 - val_loss: 0.0918 - lr: 4.9000e-04\n",
      "Epoch 166/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0918 - lr: 4.9000e-04\n",
      "Epoch 167/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0927 - lr: 4.9000e-04\n",
      "Epoch 168/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0922 - lr: 4.9000e-04\n",
      "Epoch 169/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0928 - val_loss: 0.0918 - lr: 4.9000e-04\n",
      "Epoch 170/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0931 - val_loss: 0.0926 - lr: 4.9000e-04\n",
      "Epoch 171/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0925 - lr: 4.9000e-04\n",
      "Epoch 172/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0931 - val_loss: 0.0919 - lr: 4.9000e-04\n",
      "Epoch 173/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0929 - val_loss: 0.0926 - lr: 4.9000e-04\n",
      "Epoch 174/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0921 - lr: 4.9000e-04\n",
      "Epoch 175/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0932 - val_loss: 0.0918 - lr: 4.9000e-04\n",
      "Epoch 176/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0919 - lr: 3.4300e-04\n",
      "Epoch 177/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0918 - lr: 3.4300e-04\n",
      "Epoch 178/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0928 - val_loss: 0.0919 - lr: 3.4300e-04\n",
      "Epoch 179/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0922 - lr: 3.4300e-04\n",
      "Epoch 180/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0922 - lr: 3.4300e-04\n",
      "Epoch 181/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0925 - val_loss: 0.0917 - lr: 3.4300e-04\n",
      "Epoch 182/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0926 - val_loss: 0.0918 - lr: 3.4300e-04\n",
      "Epoch 183/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0917 - lr: 3.4300e-04\n",
      "Epoch 184/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0926 - val_loss: 0.0919 - lr: 3.4300e-04\n",
      "Epoch 185/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0917 - lr: 3.4300e-04\n",
      "Epoch 186/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0926 - val_loss: 0.0919 - lr: 3.4300e-04\n",
      "Epoch 187/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0920 - lr: 3.4300e-04\n",
      "Epoch 188/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0919 - lr: 3.4300e-04\n",
      "Epoch 189/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0928 - val_loss: 0.0917 - lr: 3.4300e-04\n",
      "Epoch 190/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0918 - lr: 3.4300e-04\n",
      "Epoch 191/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0919 - lr: 2.4010e-04\n",
      "Epoch 192/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0919 - lr: 2.4010e-04\n",
      "Epoch 193/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 194/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 195/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 196/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0927 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 197/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0925 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 198/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0925 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 199/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0926 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 200/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0916 - lr: 2.4010e-04\n",
      "Epoch 201/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0920 - lr: 2.4010e-04\n",
      "Epoch 202/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 203/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0919 - lr: 2.4010e-04\n",
      "Epoch 204/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 205/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0916 - lr: 2.4010e-04\n",
      "Epoch 206/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0920 - lr: 2.4010e-04\n",
      "Epoch 207/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 208/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 209/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 210/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 211/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 212/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0918 - lr: 2.4010e-04\n",
      "Epoch 213/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 214/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0916 - lr: 2.4010e-04\n",
      "Epoch 215/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0916 - lr: 2.4010e-04\n",
      "Epoch 216/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 217/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 2.4010e-04\n",
      "Epoch 218/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0919 - lr: 2.4010e-04\n",
      "Epoch 219/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0928 - val_loss: 0.0919 - lr: 2.4010e-04\n",
      "Epoch 220/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0920 - lr: 2.4010e-04\n",
      "Epoch 221/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0918 - lr: 1.6807e-04\n",
      "Epoch 222/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 223/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 224/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0926 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 225/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0919 - lr: 1.6807e-04\n",
      "Epoch 226/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0918 - lr: 1.6807e-04\n",
      "Epoch 227/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 228/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 229/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0918 - lr: 1.6807e-04\n",
      "Epoch 230/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 231/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 232/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0918 - lr: 1.6807e-04\n",
      "Epoch 233/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 234/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0917 - lr: 1.6807e-04\n",
      "Epoch 235/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0919 - lr: 1.6807e-04\n",
      "Epoch 236/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 237/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 1.1765e-04\n",
      "Epoch 238/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 239/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0915 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 240/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 241/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0924 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 242/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 243/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 244/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0917 - lr: 1.1765e-04\n",
      "Epoch 245/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0917 - lr: 1.1765e-04\n",
      "Epoch 246/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0917 - lr: 1.1765e-04\n",
      "Epoch 247/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0923 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 248/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 1.1765e-04\n",
      "Epoch 249/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0919 - lr: 1.1765e-04\n",
      "Epoch 250/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 1.1765e-04\n",
      "Epoch 251/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 252/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 253/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0915 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 254/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 255/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 256/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 257/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0914 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 258/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 259/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0922 - val_loss: 0.0917 - lr: 8.2354e-05\n",
      "Epoch 260/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0912 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 261/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0917 - lr: 8.2354e-05\n",
      "Epoch 262/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 263/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0917 - lr: 8.2354e-05\n",
      "Epoch 264/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0915 - lr: 8.2354e-05\n",
      "Epoch 265/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0916 - lr: 8.2354e-05\n",
      "Epoch 266/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 267/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0912 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 268/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 269/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 270/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 271/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0915 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 272/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0914 - val_loss: 0.0917 - lr: 5.7648e-05\n",
      "Epoch 273/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 274/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 275/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 276/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 277/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0915 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 278/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 279/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 280/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0915 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 281/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0917 - lr: 5.7648e-05\n",
      "Epoch 282/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0917 - lr: 5.7648e-05\n",
      "Epoch 283/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 284/400\n",
      "619/619 [==============================] - 1s 995us/step - loss: 0.0915 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 285/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0915 - lr: 5.7648e-05\n",
      "Epoch 286/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0917 - lr: 5.7648e-05\n",
      "Epoch 287/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 288/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 289/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0916 - lr: 5.7648e-05\n",
      "Epoch 290/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0916 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 291/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 292/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0914 - val_loss: 0.0917 - lr: 5.0000e-05\n",
      "Epoch 293/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 5.0000e-05\n",
      "Epoch 294/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0916 - lr: 5.0000e-05\n",
      "Epoch 295/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0918 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 296/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0921 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 297/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 5.0000e-05\n",
      "Epoch 298/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0916 - lr: 5.0000e-05\n",
      "Epoch 299/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 300/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0914 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 301/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0919 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 302/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0917 - val_loss: 0.0915 - lr: 5.0000e-05\n",
      "Epoch 303/400\n",
      "619/619 [==============================] - 1s 1ms/step - loss: 0.0920 - val_loss: 0.0916 - lr: 5.0000e-05\n",
      "Epoch 304/400\n",
      "619/619 [==============================] - 1s 995us/step - loss: 0.0915 - val_loss: 0.0915 - lr: 5.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b0af8f490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_model.fit(imp_X_train, y_train, batch_size=64, epochs=400, validation_split=0.1, callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faef819",
   "metadata": {
    "id": "lXwdquLyqfKF"
   },
   "source": [
    "* Let's now evaluate our trained model, using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9501ae0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4LJVZe7qZ-F",
    "outputId": "bedcdbcd-612b-4df4-b59c-8206d588398a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 0s 700us/step - loss: 0.0917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09172344952821732"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_model.evaluate(imp_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436a839",
   "metadata": {
    "id": "a43K5pdCrCdM"
   },
   "source": [
    "* The results are very good.\n",
    "* It seems that the fact that we used a simple model in combination with a dataframe with a few but important variables achieved to capture important information and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b9a7f",
   "metadata": {
    "id": "zZ_-ql8cv3ie"
   },
   "source": [
    "## All-Columns Network\n",
    "* Following an identical simple architecture, we will create a network that will take as input our dataset with all columns.\n",
    "* Let's construct it.\n",
    "* Again, we will use **Dense** and **Batch Normalization** layers.\n",
    "* Because this one is stronger than before, we will also add a **Dropout** layer, to avoid overfit as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bb9b6db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOc33C1Vwsvi",
    "outputId": "ea8109e7-583b-44ef-dc76-94385d224a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 135)              540       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               17408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,829\n",
      "Trainable params: 28,559\n",
      "Non-trainable params: 270\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "all_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, 'sigmoid'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='softmax'),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(16),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "all_model.build((None, 135))\n",
    "all_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2b84a",
   "metadata": {
    "id": "TU6UY_Yxx1-n"
   },
   "source": [
    "* Again we will use **Adam** as our Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f3390b",
   "metadata": {
    "id": "PdzeGyKFx5zL"
   },
   "outputs": [],
   "source": [
    "all_model.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e565707",
   "metadata": {
    "id": "kk3z7HcyyDF8"
   },
   "source": [
    "* And we will do the training using the same callbacks as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8904f55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2NI5-x3yG8p",
    "outputId": "16e3b739-bdce-4fea-da60-7f9b6cea9161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "619/619 [==============================] - 2s 2ms/step - loss: 0.1350 - val_loss: 0.1154 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1159 - val_loss: 0.1037 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1109 - val_loss: 0.1009 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1073 - val_loss: 0.0993 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1061 - val_loss: 0.0959 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1033 - val_loss: 0.0940 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1019 - val_loss: 0.0935 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.1001 - val_loss: 0.0924 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0997 - val_loss: 0.0928 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0982 - val_loss: 0.0910 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0977 - val_loss: 0.0909 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0969 - val_loss: 0.0919 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0966 - val_loss: 0.0910 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0957 - val_loss: 0.0889 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0952 - val_loss: 0.0884 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0947 - val_loss: 0.0879 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0954 - val_loss: 0.0899 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0942 - val_loss: 0.0873 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0941 - val_loss: 0.0865 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0932 - val_loss: 0.0868 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0927 - val_loss: 0.0868 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0930 - val_loss: 0.0872 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0933 - val_loss: 0.0869 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0923 - val_loss: 0.0867 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0921 - val_loss: 0.0863 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0916 - val_loss: 0.0859 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0916 - val_loss: 0.0865 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0916 - val_loss: 0.0860 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0908 - val_loss: 0.0859 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0914 - val_loss: 0.0879 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0906 - val_loss: 0.0858 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0901 - val_loss: 0.0855 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0906 - val_loss: 0.0853 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0902 - val_loss: 0.0875 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0903 - val_loss: 0.0858 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0890 - val_loss: 0.0866 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0903 - val_loss: 0.0864 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0894 - val_loss: 0.0858 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0894 - val_loss: 0.0854 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0891 - val_loss: 0.0869 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0890 - val_loss: 0.0846 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0892 - val_loss: 0.0854 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0885 - val_loss: 0.0853 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0888 - val_loss: 0.0856 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0886 - val_loss: 0.0841 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0885 - val_loss: 0.0854 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0887 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0876 - val_loss: 0.0857 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0875 - val_loss: 0.0847 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0881 - val_loss: 0.0848 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0876 - val_loss: 0.0879 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0875 - val_loss: 0.0850 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0876 - val_loss: 0.0840 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0870 - val_loss: 0.0845 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0873 - val_loss: 0.0845 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0873 - val_loss: 0.0859 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0871 - val_loss: 0.0849 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0869 - val_loss: 0.0854 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0867 - val_loss: 0.0850 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0866 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0868 - val_loss: 0.0862 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0863 - val_loss: 0.0862 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0868 - val_loss: 0.0849 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0867 - val_loss: 0.0836 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0866 - val_loss: 0.0849 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0858 - val_loss: 0.0840 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0861 - val_loss: 0.0846 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0860 - val_loss: 0.0839 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0856 - val_loss: 0.0850 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0861 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0863 - val_loss: 0.0841 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0858 - val_loss: 0.0835 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0862 - val_loss: 0.0843 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0861 - val_loss: 0.0842 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0855 - val_loss: 0.0848 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0848 - val_loss: 0.0843 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0855 - val_loss: 0.0841 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0853 - val_loss: 0.0838 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0851 - val_loss: 0.0847 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0852 - val_loss: 0.0840 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0851 - val_loss: 0.0857 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0852 - val_loss: 0.0842 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0847 - val_loss: 0.0841 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0849 - val_loss: 0.0849 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0844 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0846 - val_loss: 0.0880 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0849 - val_loss: 0.0864 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0842 - val_loss: 0.0839 - lr: 7.0000e-04\n",
      "Epoch 89/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0838 - val_loss: 0.0837 - lr: 7.0000e-04\n",
      "Epoch 90/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0838 - val_loss: 0.0846 - lr: 7.0000e-04\n",
      "Epoch 91/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0842 - val_loss: 0.0840 - lr: 7.0000e-04\n",
      "Epoch 92/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0838 - val_loss: 0.0839 - lr: 7.0000e-04\n",
      "Epoch 93/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0832 - val_loss: 0.0834 - lr: 7.0000e-04\n",
      "Epoch 94/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0840 - val_loss: 0.0844 - lr: 7.0000e-04\n",
      "Epoch 95/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0838 - val_loss: 0.0877 - lr: 7.0000e-04\n",
      "Epoch 96/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0839 - val_loss: 0.0847 - lr: 7.0000e-04\n",
      "Epoch 97/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0835 - val_loss: 0.0839 - lr: 7.0000e-04\n",
      "Epoch 98/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0833 - val_loss: 0.0841 - lr: 7.0000e-04\n",
      "Epoch 99/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0832 - val_loss: 0.0844 - lr: 7.0000e-04\n",
      "Epoch 100/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0833 - val_loss: 0.0834 - lr: 7.0000e-04\n",
      "Epoch 101/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0832 - val_loss: 0.0846 - lr: 7.0000e-04\n",
      "Epoch 102/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0837 - val_loss: 0.0840 - lr: 7.0000e-04\n",
      "Epoch 103/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0832 - val_loss: 0.0836 - lr: 7.0000e-04\n",
      "Epoch 104/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0837 - val_loss: 0.0850 - lr: 7.0000e-04\n",
      "Epoch 105/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0831 - val_loss: 0.0839 - lr: 7.0000e-04\n",
      "Epoch 106/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0836 - val_loss: 0.0841 - lr: 7.0000e-04\n",
      "Epoch 107/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0833 - val_loss: 0.0836 - lr: 7.0000e-04\n",
      "Epoch 108/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0831 - val_loss: 0.0856 - lr: 7.0000e-04\n",
      "Epoch 109/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0830 - val_loss: 0.0844 - lr: 7.0000e-04\n",
      "Epoch 110/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0833 - val_loss: 0.0847 - lr: 7.0000e-04\n",
      "Epoch 111/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0827 - val_loss: 0.0836 - lr: 7.0000e-04\n",
      "Epoch 112/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0828 - val_loss: 0.0840 - lr: 7.0000e-04\n",
      "Epoch 113/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0826 - val_loss: 0.0835 - lr: 7.0000e-04\n",
      "Epoch 114/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0828 - val_loss: 0.0844 - lr: 7.0000e-04\n",
      "Epoch 115/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0824 - val_loss: 0.0836 - lr: 7.0000e-04\n",
      "Epoch 116/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0823 - val_loss: 0.0832 - lr: 4.9000e-04\n",
      "Epoch 117/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0821 - val_loss: 0.0837 - lr: 4.9000e-04\n",
      "Epoch 118/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0819 - val_loss: 0.0832 - lr: 4.9000e-04\n",
      "Epoch 119/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0821 - val_loss: 0.0839 - lr: 4.9000e-04\n",
      "Epoch 120/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0818 - val_loss: 0.0838 - lr: 4.9000e-04\n",
      "Epoch 121/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0821 - val_loss: 0.0850 - lr: 4.9000e-04\n",
      "Epoch 122/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0816 - val_loss: 0.0839 - lr: 4.9000e-04\n",
      "Epoch 123/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0825 - val_loss: 0.0836 - lr: 4.9000e-04\n",
      "Epoch 124/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0824 - val_loss: 0.0833 - lr: 4.9000e-04\n",
      "Epoch 125/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0818 - val_loss: 0.0842 - lr: 4.9000e-04\n",
      "Epoch 126/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0824 - val_loss: 0.0836 - lr: 4.9000e-04\n",
      "Epoch 127/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0816 - val_loss: 0.0838 - lr: 4.9000e-04\n",
      "Epoch 128/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0824 - val_loss: 0.0836 - lr: 4.9000e-04\n",
      "Epoch 129/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0816 - val_loss: 0.0838 - lr: 4.9000e-04\n",
      "Epoch 130/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0817 - val_loss: 0.0843 - lr: 4.9000e-04\n",
      "Epoch 131/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0818 - val_loss: 0.0836 - lr: 4.9000e-04\n",
      "Epoch 132/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0816 - val_loss: 0.0831 - lr: 3.4300e-04\n",
      "Epoch 133/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0815 - val_loss: 0.0835 - lr: 3.4300e-04\n",
      "Epoch 134/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0810 - val_loss: 0.0838 - lr: 3.4300e-04\n",
      "Epoch 135/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0808 - val_loss: 0.0832 - lr: 3.4300e-04\n",
      "Epoch 136/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0814 - val_loss: 0.0843 - lr: 3.4300e-04\n",
      "Epoch 137/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0812 - val_loss: 0.0831 - lr: 3.4300e-04\n",
      "Epoch 138/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0807 - val_loss: 0.0842 - lr: 3.4300e-04\n",
      "Epoch 139/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0815 - val_loss: 0.0842 - lr: 3.4300e-04\n",
      "Epoch 140/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0813 - val_loss: 0.0834 - lr: 3.4300e-04\n",
      "Epoch 141/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0814 - val_loss: 0.0837 - lr: 3.4300e-04\n",
      "Epoch 142/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0810 - val_loss: 0.0834 - lr: 3.4300e-04\n",
      "Epoch 143/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0815 - val_loss: 0.0841 - lr: 3.4300e-04\n",
      "Epoch 144/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0813 - val_loss: 0.0836 - lr: 3.4300e-04\n",
      "Epoch 145/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0807 - val_loss: 0.0836 - lr: 3.4300e-04\n",
      "Epoch 146/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0810 - val_loss: 0.0837 - lr: 3.4300e-04\n",
      "Epoch 147/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0809 - val_loss: 0.0849 - lr: 2.4010e-04\n",
      "Epoch 148/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0811 - val_loss: 0.0831 - lr: 2.4010e-04\n",
      "Epoch 149/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0808 - val_loss: 0.0836 - lr: 2.4010e-04\n",
      "Epoch 150/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0810 - val_loss: 0.0832 - lr: 2.4010e-04\n",
      "Epoch 151/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0804 - val_loss: 0.0833 - lr: 2.4010e-04\n",
      "Epoch 152/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0801 - val_loss: 0.0840 - lr: 2.4010e-04\n",
      "Epoch 153/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0809 - val_loss: 0.0836 - lr: 2.4010e-04\n",
      "Epoch 154/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0801 - val_loss: 0.0842 - lr: 2.4010e-04\n",
      "Epoch 155/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0805 - val_loss: 0.0834 - lr: 2.4010e-04\n",
      "Epoch 156/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0803 - val_loss: 0.0838 - lr: 2.4010e-04\n",
      "Epoch 157/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0808 - val_loss: 0.0835 - lr: 2.4010e-04\n",
      "Epoch 158/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0810 - val_loss: 0.0833 - lr: 2.4010e-04\n",
      "Epoch 159/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0808 - val_loss: 0.0841 - lr: 2.4010e-04\n",
      "Epoch 160/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0802 - val_loss: 0.0837 - lr: 2.4010e-04\n",
      "Epoch 161/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0805 - val_loss: 0.0834 - lr: 2.4010e-04\n",
      "Epoch 162/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0809 - val_loss: 0.0834 - lr: 1.6807e-04\n",
      "Epoch 163/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0805 - val_loss: 0.0845 - lr: 1.6807e-04\n",
      "Epoch 164/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0803 - val_loss: 0.0836 - lr: 1.6807e-04\n",
      "Epoch 165/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0804 - val_loss: 0.0834 - lr: 1.6807e-04\n",
      "Epoch 166/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0808 - val_loss: 0.0836 - lr: 1.6807e-04\n",
      "Epoch 167/300\n",
      "619/619 [==============================] - 1s 2ms/step - loss: 0.0807 - val_loss: 0.0832 - lr: 1.6807e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b0bdfb9d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model.fit(all_X_train, y_train, batch_size=64, epochs=300, validation_split=0.1, callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8382bc5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvToxWH1y2T7",
    "outputId": "db9943fc-fdfb-478c-b774-e930d4c68301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 0s 880us/step - loss: 0.0846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08459849655628204"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model.evaluate(all_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597a01a",
   "metadata": {
    "id": "8r4JGRNL0WeW"
   },
   "source": [
    "* The results here were pretty good again.\n",
    "* These are the best results he have achieved till here.\n",
    "* The architecture of this NN let it capture the more detailed information that was provided to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499047f",
   "metadata": {
    "id": "fvLwjUi90oXd"
   },
   "source": [
    "### RNN Network\n",
    "* In our last network we will try to **mimic the function of the human brain**, when we listen to a song.\n",
    "* In specific, we will try to take advantage of the time dimension of a song, using the segments that we have already taken, in the <code>data_preaparation.ipynb</code> notebook.\n",
    "* We will do that using an [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) layer.\n",
    "* Then, we will combine the output of this layer with the most important audio features of the songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1033d",
   "metadata": {
    "id": "kJjQ3IWE2bqR"
   },
   "source": [
    "* First, we will load the <code>tracks</code> dataframe, as well as, the <code>analysis_array_start.npy</code> file that we created in the <code>data_preaparation.ipynb</code> notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90994a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracks.csv', index_col=0)\n",
    "analysis_array_start = np.load('data/analysis_array_start.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90529f",
   "metadata": {},
   "source": [
    "* Next, we have to create the input that we will use to feed our Network.\n",
    "* So we will create a function to do that.\n",
    "* We will use a tensorflow's [from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) function.\n",
    "* The generator will yield two inputs:\n",
    "  * The one refers to the segments of the song.\n",
    "  * The second refers to the audio features of the song.\n",
    "* Here, it is important to notice that we will use only analyses data from the beginning of the songs, in order to not increase the execution time of LSTM which is already very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6b4854",
   "metadata": {
    "id": "L4FNSKTP2lq7"
   },
   "outputs": [],
   "source": [
    "rnn_track_features = np.array(df.loc[:, ['danceability', 'energy', 'loudness', 'acousticness', 'tempo', 'duration_ms']])\n",
    "y = df['valence']\n",
    "#https://stackoverflow.com/questions/63129357/passing-x-train-as-a-list-of-numpy-arrays-to-tf-data-dataset-is-not-working\n",
    "def generate_rnn_dataset():\n",
    "\n",
    "  def generator():\n",
    "    for s1, s2, l in zip(analysis_array_start, rnn_track_features, y):\n",
    "      yield {'input_1': s1, 'input_2': s2}, l\n",
    "\n",
    "  dataset = tf.data.Dataset.from_generator(generator, \n",
    "                                           output_shapes=({'input_1': (200, 28), 'input_2': (6)}, ()),\n",
    "                                           output_types=({'input_1': tf.float32, 'input_2': tf.float32}, tf.float32))\n",
    "  dataset = dataset.batch(batch_size=64)\n",
    "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4462cff",
   "metadata": {
    "id": "SFKj7UOo49Hx"
   },
   "source": [
    "* Now, we have to split our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "165a6529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAqFv4sg5CcQ",
    "outputId": "32a22fd9-7af8-466c-e5fe-8107c567f473"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 86, 172)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets\n",
    "full_rnn_dataset = generate_rnn_dataset()\n",
    "\n",
    "DATASET_SIZE = len(list(full_rnn_dataset))\n",
    "train_rnn_size = int(0.7 * DATASET_SIZE)\n",
    "val_rnn_size = int(0.1 * DATASET_SIZE)\n",
    "test_rnn_size = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "full_rnn_dataset = full_rnn_dataset.shuffle(DATASET_SIZE, seed=1)\n",
    "train_rnn_dataset = full_rnn_dataset.take(train_rnn_size)\n",
    "test_rnn_dataset = full_rnn_dataset.skip(train_rnn_size)\n",
    "val_rnn_dataset = test_rnn_dataset.skip(test_rnn_size)\n",
    "test_rnn_dataset = test_rnn_dataset.take(test_rnn_size)\n",
    "\n",
    "len(list(train_rnn_dataset)), len(list(val_rnn_dataset)), len(list(test_rnn_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9304ea8",
   "metadata": {
    "id": "iEKg4S455Zyq"
   },
   "source": [
    "* Then, we will build our Network.\n",
    "* It will have two branches, one for the segments and one for the audio features, that will merge into one.\n",
    "* Again **Batch Normalization** and **Dropout** layers will be used.\n",
    "* This archtiecture cannot be built using <code>Sequential</code>, so we will create it without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5e12c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gyvsTuq51po",
    "outputId": "e2703c60-41c7-4e69-9d27-6854261f91b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200, 28)]    0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 200, 28)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 64)           23808       ['masking[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 70)           0           ['lstm[0][0]',                   \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 70)          280         ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 70)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           2272        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           528         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            136         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            9           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,033\n",
      "Trainable params: 26,893\n",
      "Non-trainable params: 140\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "segments_input = tf.keras.Input(shape=(200, 28))\n",
    "track_input = tf.keras.Input(shape=(6))\n",
    "\n",
    "segment_analysis_branch =  tf.keras.layers.Masking(mask_value = 0.0)(segments_input)\n",
    "segment_analysis_branch = tf.keras.layers.LSTM(64)(segment_analysis_branch)\n",
    "segment_analysis_branch = Model(inputs=segments_input, outputs=segment_analysis_branch)\n",
    "\n",
    "full_track_info = tf.keras.layers.concatenate([segment_analysis_branch.output, track_input])\n",
    "full_track_info = tf.keras.layers.BatchNormalization()(full_track_info)\n",
    "full_track_info = tf.keras.layers.Dropout(0.3)(full_track_info)\n",
    "\n",
    "last_part = tf.keras.layers.Dense(32)(full_track_info)\n",
    "last_part = tf.keras.layers.Dense(16, activation='tanh')(last_part)\n",
    "last_part = tf.keras.layers.Dense(8)(last_part)\n",
    "last_part = tf.keras.layers.Dense(1, activation='sigmoid')(last_part)\n",
    "\n",
    "rnn_model = Model(inputs=[segments_input, track_input], outputs=last_part)\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64097c",
   "metadata": {
    "id": "rfWdWSGC6pGB"
   },
   "source": [
    "* We will also create a visual representation of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c232237f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2g4-A7Gx6tMQ",
    "outputId": "ed9a9564-f28a-42a9-f2b2-dc28b097bc69"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"561pt\" height=\"780pt\" viewBox=\"0.00 0.00 577.00 802.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.97 0.97) rotate(0) translate(4 798)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-798 573,-798 573,4 -4,4\"/>\n",
       "<!-- 2560452624784 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2560452624784</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-747.5 0,-793.5 305,-793.5 305,-747.5 0,-747.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"29.5\" y=\"-766.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input_1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"59,-747.5 59,-793.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"97.5\" y=\"-766.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"136,-747.5 136,-793.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-778.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"136,-770.5 192,-770.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-755.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"192,-747.5 192,-793.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-778.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 200, 28)]</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"192,-770.5 305,-770.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"248.5\" y=\"-755.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 200, 28)]</text>\n",
       "</g>\n",
       "<!-- 2560452896464 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2560452896464</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"9.5,-664.5 9.5,-710.5 295.5,-710.5 295.5,-664.5 9.5,-664.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.5\" y=\"-683.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">masking</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"71.5,-664.5 71.5,-710.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-683.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Masking</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"135.5,-664.5 135.5,-710.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-695.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"135.5,-687.5 191.5,-687.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-672.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"191.5,-664.5 191.5,-710.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5\" y=\"-695.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 200, 28)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"191.5,-687.5 295.5,-687.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5\" y=\"-672.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 200, 28)</text>\n",
       "</g>\n",
       "<!-- 2560452624784&#45;&gt;2560452896464 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2560452624784-&gt;2560452896464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.5,-747.37C152.5,-739.15 152.5,-729.66 152.5,-720.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156,-720.61 152.5,-710.61 149,-720.61 156,-720.61\"/>\n",
       "</g>\n",
       "<!-- 2560452897232 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2560452897232</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"25.5,-581.5 25.5,-627.5 279.5,-627.5 279.5,-581.5 25.5,-581.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lstm</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"65.5,-581.5 65.5,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"92.5\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTM</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"119.5,-581.5 119.5,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"119.5,-604.5 175.5,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"147.5\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"175.5,-581.5 175.5,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"227.5\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 200, 28)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"175.5,-604.5 279.5,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"227.5\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2560452896464&#45;&gt;2560452897232 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2560452896464-&gt;2560452897232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.5,-664.37C152.5,-656.15 152.5,-646.66 152.5,-637.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156,-637.61 152.5,-627.61 149,-637.61 156,-637.61\"/>\n",
       "</g>\n",
       "<!-- 2560453653264 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2560453653264</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"107,-498.5 107,-544.5 478,-544.5 478,-498.5 107,-498.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.5\" y=\"-517.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">concatenate</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"190,-498.5 190,-544.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"233\" y=\"-517.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Concatenate</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-498.5 276,-544.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"304\" y=\"-529.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-521.5 332,-521.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"304\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"332,-498.5 332,-544.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"405\" y=\"-529.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 64), (None, 6)]</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"332,-521.5 478,-521.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"405\" y=\"-506.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "</g>\n",
       "<!-- 2560452897232&#45;&gt;2560453653264 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2560452897232-&gt;2560453653264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M190.76,-581.37C207.58,-571.63 227.5,-560.11 245.24,-549.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.39,-552.64 254.29,-544.61 243.88,-546.59 247.39,-552.64\"/>\n",
       "</g>\n",
       "<!-- 2560452624496 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2560452624496</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"298,-581.5 298,-627.5 569,-627.5 569,-581.5 298,-581.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"327.5\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input_2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"357,-581.5 357,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-600.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">InputLayer</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"434,-581.5 434,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"462\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"434,-604.5 490,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"462\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"490,-581.5 490,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"529.5\" y=\"-612.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 6)]</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"490,-604.5 569,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"529.5\" y=\"-589.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[(None, 6)]</text>\n",
       "</g>\n",
       "<!-- 2560452624496&#45;&gt;2560453653264 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2560452624496-&gt;2560453653264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.97,-581.37C378.03,-571.63 357.96,-560.11 340.1,-549.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341.4,-546.55 330.98,-544.61 337.91,-552.62 341.4,-546.55\"/>\n",
       "</g>\n",
       "<!-- 2560452898096 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2560452898096</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"98.5,-415.5 98.5,-461.5 486.5,-461.5 486.5,-415.5 98.5,-415.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-434.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">batch_normalization</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"227.5,-415.5 227.5,-461.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"290.5\" y=\"-434.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">BatchNormalization</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"353.5,-415.5 353.5,-461.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"381.5\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"353.5,-438.5 409.5,-438.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"381.5\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"409.5,-415.5 409.5,-461.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"448\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"409.5,-438.5 486.5,-438.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"448\" y=\"-423.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "</g>\n",
       "<!-- 2560453653264&#45;&gt;2560452898096 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2560453653264-&gt;2560452898096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-498.37C292.5,-490.15 292.5,-480.66 292.5,-471.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-471.61 292.5,-461.61 289,-471.61 296,-471.61\"/>\n",
       "</g>\n",
       "<!-- 2560591236448 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2560591236448</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"163,-332.5 163,-378.5 422,-378.5 422,-332.5 163,-332.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"194\" y=\"-351.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dropout</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"225,-332.5 225,-378.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"257\" y=\"-351.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dropout</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"289,-332.5 289,-378.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"317\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"289,-355.5 345,-355.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"317\" y=\"-340.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"345,-332.5 345,-378.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-363.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"345,-355.5 422,-355.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"383.5\" y=\"-340.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "</g>\n",
       "<!-- 2560452898096&#45;&gt;2560591236448 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2560452898096-&gt;2560591236448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-415.37C292.5,-407.15 292.5,-397.66 292.5,-388.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-388.61 292.5,-378.61 289,-388.61 296,-388.61\"/>\n",
       "</g>\n",
       "<!-- 2560454327024 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2560454327024</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"176,-249.5 176,-295.5 409,-295.5 409,-249.5 176,-249.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"225,-249.5 225,-295.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-268.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-249.5 276,-295.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"304\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"276,-272.5 332,-272.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"304\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"332,-249.5 332,-295.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"370.5\" y=\"-280.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 70)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"332,-272.5 409,-272.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"370.5\" y=\"-257.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 2560591236448&#45;&gt;2560454327024 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2560591236448-&gt;2560454327024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-332.37C292.5,-324.15 292.5,-314.66 292.5,-305.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-305.61 292.5,-295.61 289,-305.61 296,-305.61\"/>\n",
       "</g>\n",
       "<!-- 2560454255664 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2560454255664</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"169,-166.5 169,-212.5 416,-212.5 416,-166.5 169,-166.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_1</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"232,-166.5 232,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"257.5\" y=\"-185.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"283,-166.5 283,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"283,-189.5 339,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"339,-166.5 339,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-197.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 32)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"339,-189.5 416,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-174.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 2560454327024&#45;&gt;2560454255664 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2560454327024-&gt;2560454255664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-249.37C292.5,-241.15 292.5,-231.66 292.5,-222.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-222.61 292.5,-212.61 289,-222.61 296,-222.61\"/>\n",
       "</g>\n",
       "<!-- 2560591400816 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2560591400816</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"169,-83.5 169,-129.5 416,-129.5 416,-83.5 169,-83.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_2</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"232,-83.5 232,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"257.5\" y=\"-102.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"283,-83.5 283,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"283,-106.5 339,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"339,-83.5 339,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-114.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 16)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"339,-106.5 416,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 2560454255664&#45;&gt;2560591400816 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2560454255664-&gt;2560591400816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-166.37C292.5,-158.15 292.5,-148.66 292.5,-139.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-139.61 292.5,-129.61 289,-139.61 296,-139.61\"/>\n",
       "</g>\n",
       "<!-- 2560591397888 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2560591397888</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"172.5,-0.5 172.5,-46.5 412.5,-46.5 412.5,-0.5 172.5,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dense_3</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"235.5,-0.5 235.5,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"286.5,-0.5 286.5,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"286.5,-23.5 342.5,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"314.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"342.5,-0.5 342.5,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-31.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 8)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"342.5,-23.5 412.5,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2560591400816&#45;&gt;2560591397888 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2560591400816-&gt;2560591397888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.5,-83.37C292.5,-75.15 292.5,-65.66 292.5,-56.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"296,-56.61 292.5,-46.61 289,-56.61 296,-56.61\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(rnn_model, show_shapes=True, dpi=70).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5aef4f",
   "metadata": {
    "id": "4op0YjQ561cq"
   },
   "source": [
    "* Here, the training was extremely demanding.\n",
    "* More than 100 epochs of training were executed.\n",
    "* Each of them needed more than 10 minutes on Google Colab.\n",
    "* So it is easy to understand that the training was split.\n",
    "* In the <code>data/rnn_network/rnn_final</code> folder can be found the trained network, and we will load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cd53d72",
   "metadata": {
    "id": "CClkOh0o6xB8"
   },
   "outputs": [],
   "source": [
    "rnn_model = tf.keras.models.load_model('data/rnn_network/rnn_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bebbaa",
   "metadata": {
    "id": "0oeAvCtz9uJd"
   },
   "source": [
    "* And we will do the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576317df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2XIVAgO8TPY",
    "outputId": "e5b821fa-9166-4ac4-e716-8a0dc18ca24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 10s 48ms/step - loss: 0.0934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09339161962270737"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.evaluate(test_rnn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336eaf3",
   "metadata": {
    "id": "b4N7QB0b9sq1"
   },
   "source": [
    "* Here the result is good, but not as good as before. Possibly, this network is too strong for the size of the dataset that we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165b152",
   "metadata": {},
   "source": [
    "* So the best results where achieved by the second, **All Columns Neural Network**, and its *Mean Absolute Error* on the test set was **0.0846**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
